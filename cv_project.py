# -*- coding: utf-8 -*-
"""CV project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FdNxkAZXeqEO560Izn8Hbbv2844TC1kV
"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!pip install -q kaggle

!kaggle datasets download -d emmarex/plantdisease

!unzip -q plantdisease.zip -d plant_disease

import tensorflow as tf
from tensorflow.keras import layers, models, Input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import os

IMAGE_SIZE = (128, 128)
BATCH_SIZE = 32
NUM_CLASSES = 15
EPOCHS = 10

DATASET_PATH = 'plant_disease/PlantVillage'

datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    horizontal_flip=True,
    zoom_range=0.2,
    rotation_range=15
)

train_gen = datagen.flow_from_directory(
    DATASET_PATH,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

val_gen = datagen.flow_from_directory(
    DATASET_PATH,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation',
    shuffle=True
)

from tensorflow.keras import layers, models, Input

def attention_block(inputs):
    channels = inputs.shape[-1]

    attention = layers.GlobalAveragePooling2D()(inputs)
    attention = layers.Dense(channels // 4, activation='relu')(attention)  # More expressive
    attention = layers.Dense(channels, activation='sigmoid')(attention)
    attention = layers.Reshape((1, 1, channels))(attention)

    return layers.Multiply()([inputs, attention])

def build_model(input_shape, num_classes):
    inputs = Input(shape=input_shape)

    # Block 1
    x = layers.Conv2D(32, (3, 3), padding='same')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = attention_block(x)

    # Block 2
    x = layers.Conv2D(64, (3, 3), padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = attention_block(x)

    # Block 3
    x = layers.Conv2D(128, (3, 3), padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = attention_block(x)

    # Block 4 (optional deeper layer)
    x = layers.Conv2D(256, (3, 3), padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = attention_block(x)

    # Classification head
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(0.5)(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    model = models.Model(inputs, outputs)
    return model

model = build_model((*IMAGE_SIZE, 3), NUM_CLASSES)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=EPOCHS
)

loss, accuracy = model.evaluate(val_gen)
print(f"Validation Accuracy: {accuracy * 100:.2f}%")

import numpy as np
import matplotlib.pyplot as plt

x_batch, y_batch = next(val_gen)

predictions = model.predict(x_batch)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(y_batch, axis=1)
class_labels = list(train_gen.class_indices.keys())

for i in range(5):
    plt.imshow(x_batch[i])
    plt.title(f"True: {class_labels[true_classes[i]]} | Predicted: {class_labels[predicted_classes[i]]}")
    plt.axis('off')
    plt.show()
